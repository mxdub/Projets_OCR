{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-54c89494-6eac-4b50-a600-718e45e09320;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 730ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-54c89494-6eac-4b50-a600-718e45e09320\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/05 13:47:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:3.3.2 pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Fruits').getOrCreate()\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3-eu-west-3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\n",
    "\n",
    "rdd = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\",\"true\").option(\"dropInvalid\", True).load(\"s3a://ocr-mxdub/test_data/\")\n",
    "rdd = rdd.select(['content', 'path']).rdd.map(lambda x : (x[0], x[1].split('/')[4] )).toDF(['Image', 'Label'])\n",
    "\n",
    "print(\"PARTITIONS : {}\".format(rdd.rdd.getNumPartitions()))\n",
    "rdd = rdd.coalesce(2)\n",
    "print(\"PARTITIONS : {}\".format(rdd.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Needed PANDAS_UDF\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "model = VGG16()\n",
    "model_1 = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "bc_model_weights = spark.sparkContext.broadcast(model_1.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a VGG16 model with top layer removed and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = VGG16(weights=None)\n",
    "    model_1 = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    # model = ResNet50(weights=None, include_top=False)\n",
    "    model_1.set_weights(bc_model_weights.value)\n",
    "    return model_1\n",
    "\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "\n",
    "@pandas_udf('array<float>')\n",
    "def featurize_udf(content_series_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n",
    "    \n",
    "# Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors.\n",
    "# If you hit such errors in the cell below, try reducing the Arrow batch size via `maxRecordsPerBatch`.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")\n",
    "\n",
    "features_df = rdd.select(col(\"Label\"), featurize_udf(\"Image\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b5c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last layer output size (i.e. # of features)\n",
    "feats_size_VGG16 = model_1.layers[-1].output.shape[1]\n",
    "\n",
    "# Transform as wide df & save\n",
    "features_long = features_df.select([col(\"Label\")] + [(col(\"features\")[i]).alias(\"feats_{}\".format(i)) for i in range(feats_size_VGG16)])\n",
    "features_long.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"s3a://ocr-mxdub/results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
